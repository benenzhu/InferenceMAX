name: Template - Benchmark
on:
  workflow_call:
    inputs:
      runner:
        required: true
        type: string
      image:
        required: true
        type: string
      model:
        required: true
        type: string
      model-prefix:
        required: true
        type: string
      precision:
        required: true
        type: string
      framework:
        required: true
        type: string
      exp-name:
        required: true
        type: string
      isl:
        required: true
        type: string
      osl:
        required: true
        type: string
      tp:
        required: true
        type: string
      ep:
        required: true
        type: string
      dp-attn:
        required: true
        type: boolean
      max-model-len:
        required: true
        type: string
      conc:
        required: true
        type: string
      spec-decoding:
        required: true
        type: string
      disagg:
        required: true
        type: string
      run-eval:
        type: boolean
        required: true
        default: false
      random-range-ratio:
        required: false
        type: string
        default: '0.8'
      ref:
        description: "Git ref (branch/sha) to checkout"
        required: false
        type: string

env:
  RANDOM_RANGE_RATIO: 0.8
  HF_TOKEN: ${{ secrets.HF_TOKEN }}
  HF_HUB_CACHE: '/mnt/hf_hub_cache/'
  EXP_NAME: ${{ inputs.exp-name }}
  MODEL: ${{ inputs.model }}
  MODEL_PREFIX: ${{ inputs.model-prefix }}
  ISL: ${{ inputs.isl }}
  OSL: ${{ inputs.osl }}
  MAX_MODEL_LEN: ${{ inputs.max-model-len }}
  IMAGE: ${{ inputs.image }}
  FRAMEWORK: ${{ inputs.framework }}
  PRECISION: ${{ inputs.precision }}
  TP: ${{ inputs.tp }}
  EP_SIZE: ${{ inputs.ep }}
  DP_ATTENTION: ${{ inputs.dp-attn }}
  CONC: ${{ inputs.conc }}
  SPEC_DECODING: ${{ inputs.spec-decoding }}
  DISAGG: ${{ inputs.disagg }}
  RUN_EVAL: ${{ inputs.run-eval }}

permissions:
  contents: read

jobs:
  benchmark:
    runs-on: ${{ inputs.runner }}
    timeout-minutes: 180
    name: "${{ inputs.exp-name }} ${{ inputs.runner }} ${{ inputs.framework }} ${{ inputs.precision }} ${{ inputs.run-eval && 'eval ' || '' }}tp=${{ inputs.tp }} ep=${{ inputs.ep }} dpa=${{ inputs.dp-attn }} conc=${{ inputs.conc }} spec=${{ inputs.spec-decoding }}"
    steps:
      - name: Resource cleanup (pre-run)
        run: &resource-cleanup |
          # Cleanup Docker resources
          if command -v docker >/dev/null 2>&1 && docker info >/dev/null 2>&1; then
            echo "[Docker] Cleaning up resources ..."
            docker ps -aq | xargs -r docker rm -f
            docker network prune -f
            while [ -n "$(docker ps -aq)" ]; do
              docker ps -a
              sleep 5
            done
          fi

          # Cleanup SLURM resources
          if command -v squeue >/dev/null 2>&1; then
            if [[ "${{ runner.name }}" == mi355x-amds* ]]; then
              echo "[Slurm] Cleaning up jobs with name: ${{ runner.name }} ..."
              scancel --name="${{ runner.name }}" || true
              while [ -n "$(squeue --name='${{ runner.name }}' --noheader --format='%i')" ]; do
                squeue --name="${{ runner.name }}"
                sleep 5
              done
            else
              echo "[Slurm] Cleaning up jobs for user: $USER ..."
              scancel -u "$USER" || true
              while [ -n "$(squeue -u "$USER" --noheader --format='%i')" ]; do
                squeue -u "$USER"
                sleep 5
              done
            fi
          fi

      - uses: actions/checkout@de0fac2e4500dabe0009e67214ff5f5447ce83dd # v6.0.2
        with:
          token: ${{ secrets.REPO_PAT }}
          fetch-depth: 0
          ref: ${{ inputs.ref || github.ref }}

      - name: Launch job script
        env:
          RUNNER_NAME: ${{ runner.name }}
          RUNNER_TYPE: ${{ inputs.runner }}
          RESULT_FILENAME: ${{ env.EXP_NAME }}_${{ env.PRECISION }}_${{ env.FRAMEWORK }}_tp${{ env.TP }}_ep${{ env.EP_SIZE }}_dpa_${{ env.DP_ATTENTION }}_conc${{ env.CONC }}_specdecode_${{ env.SPEC_DECODING }}_${{ runner.name }}
          # Suppress per-job eval markdown from being appended to the step summary.
          # We'll publish a single combined eval table in the collection job instead.
          GITHUB_STEP_SUMMARY: ''
        run: |
          bash ./runners/launch_${RUNNER_NAME%%_*}.sh
          FOUND_RESULT_FILE=
          for i in {1..10}; do
            if [ -f "$RESULT_FILENAME.json" ]; then
              echo "RESULT_FILENAME=${RESULT_FILENAME}" >> $GITHUB_ENV
              FOUND_RESULT_FILE=true
              break
            fi
            echo "Waiting for result file... (attempt $i)"
            sleep 1
          done
   
          if [ -z "$FOUND_RESULT_FILE" ]; then
            echo "Run failed: Benchmark result $RESULT_FILENAME.json not found." >&2
            exit 1
          fi

      - name: Process result
        env:
          RUNNER_TYPE: ${{ inputs.runner }}
        run: |
          python3 utils/process_result.py

      - name: Upload result
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: bmk_${{ env.RESULT_FILENAME }}
          path: agg_${{ env.RESULT_FILENAME }}.json
        
      - name: Upload eval results (if any)
        if: ${{ env.RUN_EVAL == 'true' }}
        uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v6.0.0
        with:
          name: eval_${{ env.EXP_NAME }}_${{ env.RESULT_FILENAME }}
          path: |
            meta_env.json
            results*.json
            sample*.jsonl
          if-no-files-found: ignore

      - name: Cleanup eval outputs (post-upload)
        if: ${{ env.RUN_EVAL == 'true' }}
        run: |
          rm -f meta_env.json || true
          # Remove any eval results JSONs that were moved into workspace
          rm -f results*.json || true
          
      - name: Resource cleanup (post-run)
        if: always()
        run: *resource-cleanup
